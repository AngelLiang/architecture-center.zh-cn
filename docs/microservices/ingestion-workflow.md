---
title: '微服务中的引流和工作流 '
description: '微服务中的引流和工作流 '
author: MikeWasson
ms.date: 10/23/2018
ms.topic: guide
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: microservices
ms.openlocfilehash: a36d2b4c7bfd2b26d5e1de44ddd8005fbce4bdd2
ms.sourcegitcommit: 579c39ff4b776704ead17a006bf24cd4cdc65edd
ms.translationtype: MT
ms.contentlocale: zh-CN
ms.lasthandoff: 04/17/2019
ms.locfileid: "59640849"
---
# <a name="designing-microservices-ingestion-and-workflow"></a><span data-ttu-id="9f6ac-103">设计微服务：引入和工作流</span><span class="sxs-lookup"><span data-stu-id="9f6ac-103">Designing microservices: Ingestion and workflow</span></span>

<span data-ttu-id="9f6ac-104">微服务通常有一个跨越多个服务（用于处理单个事务）的工作流。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-104">Microservices often have a workflow that spans multiple services for a single transaction.</span></span> <span data-ttu-id="9f6ac-105">该工作流必须可靠；它不能丢失事务，或者将事务保留为部分完成状态。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-105">The workflow must be reliable; it can't lose transactions or leave them in a partially completed state.</span></span> <span data-ttu-id="9f6ac-106">控制传入请求的引流速率至关重要。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-106">It's also critical to control the ingestion rate of incoming requests.</span></span> <span data-ttu-id="9f6ac-107">当许多的小型服务相互通信时，传入请求的剧增可能会使服务间的通信变瘫痪。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-107">With many small services communicating with each other, a burst of incoming requests can overwhelm the interservice communication.</span></span>

![引入工作流图](./images/ingestion-workflow.png)

> [!NOTE]
> <span data-ttu-id="9f6ac-109">这篇文章基于微服务引用实现调用[无人机交付应用程序](./design/index.md)。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-109">This article is based on a microservices reference implementation called the [Drone Delivery application](./design/index.md).</span></span>

## <a name="the-drone-delivery-workflow"></a><span data-ttu-id="9f6ac-110">无人机交付工作流</span><span class="sxs-lookup"><span data-stu-id="9f6ac-110">The drone delivery workflow</span></span>

<span data-ttu-id="9f6ac-111">在无人机交付应用程序中，必须执行以下操作来安排交付：</span><span class="sxs-lookup"><span data-stu-id="9f6ac-111">In the Drone Delivery application, the following operations must be performed to schedule a delivery:</span></span>

1. <span data-ttu-id="9f6ac-112">检查客户帐户的状态（帐户服务）。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-112">Check the status of the customer's account (Account service).</span></span>
2. <span data-ttu-id="9f6ac-113">创建新的包裹实体（包裹服务）。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-113">Create a new package entity (Package service).</span></span>
3. <span data-ttu-id="9f6ac-114">根据取件和交付地点，检查此项交付是否需要与任何第三方货运公司合作（第三方运输服务）。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-114">Check whether any third-party transportation is required for this delivery, based on the pickup and delivery locations (Third-party Transportation service).</span></span>
4. <span data-ttu-id="9f6ac-115">安排无人机取件（无人机服务）。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-115">Schedule a drone for pickup (Drone service).</span></span>
5. <span data-ttu-id="9f6ac-116">创建新的交付实体（交付服务）。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-116">Create a new delivery entity (Delivery service).</span></span>

<span data-ttu-id="9f6ac-117">这是整个应用程序的核心，因此，端到端的过程必须高效且可靠。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-117">This is the core of the entire application, so the end-to-end process must be performant as well as reliable.</span></span> <span data-ttu-id="9f6ac-118">必须解决一些特殊的难题：</span><span class="sxs-lookup"><span data-stu-id="9f6ac-118">Some particular challenges must be addressed:</span></span>

- <span data-ttu-id="9f6ac-119">**负载调节**。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-119">**Load leveling**.</span></span> <span data-ttu-id="9f6ac-120">过多客户端请求产生的服务间网络流量可能会使系统瘫痪。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-120">Too many client requests can overwhelm the system with interservice network traffic.</span></span> <span data-ttu-id="9f6ac-121">此外，还可能会使存储或远程服务等后端依赖项变瘫痪。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-121">It can also overwhelm backend dependencies such as storage or remote services.</span></span> <span data-ttu-id="9f6ac-122">这些依赖项做出的反应可能是限制调用它们的服务，从而在系统中产生反压。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-122">These may react by throttling the services calling them, creating backpressure in the system.</span></span> <span data-ttu-id="9f6ac-123">因此，必须将传入系统的请求放入缓冲区或队列进行处理，以便对请求进行负载调节。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-123">Therefore, it's important to load level the requests coming into the system, by putting them into a buffer or queue for processing.</span></span>

- <span data-ttu-id="9f6ac-124">**有保证的传递**。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-124">**Guaranteed delivery**.</span></span> <span data-ttu-id="9f6ac-125">为了避免丢弃任何客户端请求，引流组件必须保证至少传递消息一次。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-125">To avoid dropping any client requests, the ingestion component must guarantee at-least-once delivery of messages.</span></span>

- <span data-ttu-id="9f6ac-126">**错误处理**。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-126">**Error handling**.</span></span> <span data-ttu-id="9f6ac-127">如果有任何服务返回错误代码或者遇到非暂时性故障，则无法安排交付。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-127">If any of the services returns an error code or experiences a non-transient failure, the delivery cannot be scheduled.</span></span> <span data-ttu-id="9f6ac-128">错误代码可能指示预期的错误状态（例如，客户帐户处于冻结状态）或意外的服务器错误 (HTTP 5xx)。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-128">An error code might indicate an expected error condition (for example, the customer's account is suspended) or an unexpected server error (HTTP 5xx).</span></span> <span data-ttu-id="9f6ac-129">也有可能服务不可用，导致网络调用超时。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-129">A service might also be unavailable, causing the network call to time out.</span></span>

<span data-ttu-id="9f6ac-130">首先，让我们探讨等式的引流端 &mdash; 系统如何以较高的吞吐量引流传入的用户请求。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-130">First we'll look at the ingestion side of the equation &mdash; how the system can ingest incoming user requests at high throughput.</span></span> <span data-ttu-id="9f6ac-131">然后，我们考虑无人机交付应用程序如何实现可靠的工作流。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-131">Then we'll consider how the drone delivery application can implement a reliable workflow.</span></span> <span data-ttu-id="9f6ac-132">事实证明，引流子系统的设计会影响工作流后端。 </span><span class="sxs-lookup"><span data-stu-id="9f6ac-132">It turns out that the design of the ingestion subsystem affects the workflow backend.</span></span>

## <a name="ingestion"></a><span data-ttu-id="9f6ac-133">引流 </span><span class="sxs-lookup"><span data-stu-id="9f6ac-133">Ingestion</span></span>

<span data-ttu-id="9f6ac-134">根据业务要求，开发团队确定了与引流相关的以下非功能性要求：</span><span class="sxs-lookup"><span data-stu-id="9f6ac-134">Based on business requirements, the development team identified the following non-functional requirements for ingestion:</span></span>

- <span data-ttu-id="9f6ac-135">每秒 10000 个请求的持续性吞吐量。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-135">Sustained throughput of 10K requests/sec.</span></span>
- <span data-ttu-id="9f6ac-136">在高峰期每秒能够处理多达 50000 个请求，且不会丢弃客户端请求或导致超时。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-136">Able to handle spikes of up to 50K/sec without dropping client requests or timing out.</span></span>
- <span data-ttu-id="9f6ac-137">在 99% 的时间内延迟小于 500 毫秒。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-137">Less than 500ms latency in the 99th percentile.</span></span>

<span data-ttu-id="9f6ac-138">处理偶发性流量高峰的要求为设计带来了挑战。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-138">The requirement to handle occasional spikes in traffic presents a design challenge.</span></span> <span data-ttu-id="9f6ac-139">从理论上讲，系统可以横向扩展，以处理最大预期流量。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-139">In theory, the system could be scaled out to handle the maximum expected traffic.</span></span> <span data-ttu-id="9f6ac-140">但是，预配这么多的资源是极其低效的做法。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-140">However, provisioning that many resources would be very inefficient.</span></span> <span data-ttu-id="9f6ac-141">大多数情况下，应用程序并不需要这么大的容量，因此存在闲置的核心，造成资金浪费。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-141">Most of the time, the application will not need that much capacity, so there would be idle cores, costing money without adding value.</span></span>

<span data-ttu-id="9f6ac-142">更好的做法是将传入请求放入缓冲区，并让充当负载调节器。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-142">A better approach is to put the incoming requests into a buffer, and let the buffer act as a load leveler.</span></span> <span data-ttu-id="9f6ac-143">如果采用这种设计，引流服务必须能够在短时间内应对最大引流速率，但后端服务只需处理最大持续负载。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-143">With this design, the Ingestion service must be able to handle the maximum ingestion rate over short periods, but the backend services only need to handle the maximum sustained load.</span></span> <span data-ttu-id="9f6ac-144">如果在前端缓冲请求，则后端服务应该不需要处理较大的流量高峰。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-144">By buffering at the front end, the backend services shouldn't need to handle large spikes in traffic.</span></span> <span data-ttu-id="9f6ac-145">根据无人机交付应用程序的规模要求，比较适合使用 [Azure 事件中心](/azure/event-hubs/)进行负载调节。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-145">At the scale required for the Drone Delivery application, [Azure Event Hubs](/azure/event-hubs/) is a good choice for load leveling.</span></span> <span data-ttu-id="9f6ac-146">事件中心提供较低的延迟和较高的吞吐量，是能够处理较高引流量的经济高效解决方案。 </span><span class="sxs-lookup"><span data-stu-id="9f6ac-146">Event Hubs offers low latency and high throughput, and is a cost effective solution at high ingestion volumes.</span></span>

<span data-ttu-id="9f6ac-147">在测试中，我们使用了标准层事件中心，其中包括 32 个分区和 100 个吞吐量单位。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-147">For our testing, we used a Standard tier event hub with 32 partitions and 100 throughput units.</span></span> <span data-ttu-id="9f6ac-148">我们已观察到，引流速率大约为 32000 个事件/秒，延迟大约为 90 毫秒。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-148">We observed about 32K events / second ingestion, with latency around 90ms.</span></span> <span data-ttu-id="9f6ac-149">目前的默认限制为 20 个吞吐量单位，但 Azure 客户可以通过填写支持请求来请求更多的吞吐量单位。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-149">Currently the default limit is 20 throughput units, but Azure customers can request additional throughput units by filing a support request.</span></span> <span data-ttu-id="9f6ac-150">有关详细信息，请参阅[事件中心配额](/azure/event-hubs/event-hubs-quotas)。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-150">See [Event Hubs quotas](/azure/event-hubs/event-hubs-quotas) for more information.</span></span> <span data-ttu-id="9f6ac-151">与所有性能指标一样，许多因素会影响性能，例如消息有效负载大小，因此，请不要将这些数字视为基准。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-151">As with all performance metrics, many factors can affect performance, such as message payload size, so don't interpret these numbers as a benchmark.</span></span> <span data-ttu-id="9f6ac-152">如果需要更大的吞吐量，引流服务可以在多个事件中心之间分片。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-152">If more throughput is needed, the Ingestion service can shard across more than one event hub.</span></span> <span data-ttu-id="9f6ac-153">要进一步提高吞吐率，[事件中心专用版](/azure/event-hubs/event-hubs-dedicated-overview)可提供单租户部署，此类部署每秒可以引流 200 万以上的事件。 </span><span class="sxs-lookup"><span data-stu-id="9f6ac-153">For even higher throughput rates, [Event Hubs Dedicated](/azure/event-hubs/event-hubs-dedicated-overview) offers single-tenant deployments that can ingress over 2 million events per second.</span></span>

<span data-ttu-id="9f6ac-154">必须了解事件中心如何实现这么高的吞吐量，因为这会影响到客户端使用事件中心内的消息的方式。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-154">It's important to understand how Event Hubs can achieve such high throughput, because that affects how a client should consume messages from Event Hubs.</span></span> <span data-ttu-id="9f6ac-155">事件中心不实施队列，</span><span class="sxs-lookup"><span data-stu-id="9f6ac-155">Event Hubs does not implement a *queue*.</span></span> <span data-ttu-id="9f6ac-156">而是实施事件流。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-156">Rather, it implements an *event stream*.</span></span>

<span data-ttu-id="9f6ac-157">使用队列时，单个使用者可能会从队列中删除消息，而下一个使用者将看不到该消息。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-157">With a queue, an individual consumer can remove a message from the queue, and the next consumer won't see that message.</span></span> <span data-ttu-id="9f6ac-158">因此，借助队列可以使用[使用者竞争模式](../patterns/competing-consumers.md)来并行处理消息和提高可伸缩性。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-158">Queues therefore allow you to use a [Competing Consumers pattern](../patterns/competing-consumers.md) to process messages in parallel and improve scalability.</span></span> <span data-ttu-id="9f6ac-159">为了提高弹性，使用者可在消息上保留一把锁，并在处理完该消息后释放该锁。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-159">For greater resiliency, the consumer holds a lock on the message and releases the lock when it's done processing the message.</span></span> <span data-ttu-id="9f6ac-160">如果使用者发生故障 &mdash; 例如，它运行所在的节点崩溃 &mdash; 该锁将会超时，而消息将返回到队列中。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-160">If the consumer fails &mdash; for example, the node it's running on crashes &mdash; the lock times out and the message goes back onto the queue.</span></span>

![队列语义图](./images/queue-semantics.png)

<span data-ttu-id="9f6ac-162">在另一方面，事件中心使用流语义。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-162">Event Hubs, on the other hand, uses streaming semantics.</span></span> <span data-ttu-id="9f6ac-163">使用者根据自身的步调独立读取流。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-163">Consumers read the stream independently at their own pace.</span></span> <span data-ttu-id="9f6ac-164">每个使用者负责跟踪它当前在流中的位置。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-164">Each consumer is responsible for keeping track of its current position in the stream.</span></span> <span data-ttu-id="9f6ac-165">使用者应该根据某种预定义的间隔将其当前位置写入持久性存储。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-165">A consumer should write its current position to persistent storage at some predefined interval.</span></span> <span data-ttu-id="9f6ac-166">这样，如果使用者遇到故障（例如，使用者崩溃，或主机故障），则新实例可以继续从上一个记录的位置读取流。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-166">That way, if the consumer experiences a fault (for example, the consumer crashes, or the host fails), then a new instance can resume reading the stream from the last recorded position.</span></span> <span data-ttu-id="9f6ac-167">此过程称为“检查点设置”。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-167">This process is called *checkpointing*.</span></span>

<span data-ttu-id="9f6ac-168">出于性能原因，使用者通常不会在每条消息的后面设置检查点，</span><span class="sxs-lookup"><span data-stu-id="9f6ac-168">For performance reasons, a consumer generally doesn't checkpoint after each message.</span></span> <span data-ttu-id="9f6ac-169">而是根据某个固定的间隔设置检查点，例如，在处理 n 条消息之后或者每隔 n 秒设置检查点。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-169">Instead, it checkpoints at some fixed interval, for example after processing *n* messages, or every *n* seconds.</span></span> <span data-ttu-id="9f6ac-170">因此，如果某个使用者发生故障，则某些事件可能会处理两次，因为新实例始终从最后一个检查点拾取消息。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-170">As a consequence, if a consumer fails, some events may get processed twice, because a new instance always picks up from the last checkpoint.</span></span> <span data-ttu-id="9f6ac-171">利弊：密集的检查点可能会降低性能，但设置稀疏的检查点则意味着在发生故障后需要重放更多事件。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-171">There is a tradeoff: Frequent checkpoints can hurt performance, but sparse checkpoints mean you will replay more events after a failure.</span></span>

![流语义图](./images/stream-semantics.png)

<span data-ttu-id="9f6ac-173">事件中心不是针对竞争性使用者设计的。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-173">Event Hubs is not designed for competing consumers.</span></span> <span data-ttu-id="9f6ac-174">尽管多个使用者可以读取某个流，但每个使用者需要独立遍历该流。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-174">Although multiple consumers can read a stream, each traverses the stream independently.</span></span> <span data-ttu-id="9f6ac-175">事件中心使用分区使用者模式。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-175">Instead, Event Hubs uses a partitioned consumer pattern.</span></span> <span data-ttu-id="9f6ac-176">一个事件中心最多包含 32 个分区。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-176">An event hub has up to 32 partitions.</span></span> <span data-ttu-id="9f6ac-177">可以通过将一个独立的使用者分配到每个分区进行横向缩放。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-177">Horizontal scale is achieved by assigning a separate consumer to each partition.</span></span>

<span data-ttu-id="9f6ac-178">对于无人机交付工作流而言，这意味着什么？</span><span class="sxs-lookup"><span data-stu-id="9f6ac-178">What does this mean for the drone delivery workflow?</span></span> <span data-ttu-id="9f6ac-179">为了充分利用事件中心的优势，交付计划程序不能等到处理完每条消息之后才转移到下一条消息。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-179">To get the full benefit of Event Hubs, the Delivery Scheduler cannot wait for each message to be processed before moving onto the next.</span></span> <span data-ttu-id="9f6ac-180">否则，它要将大部分时间花费在等待网络调用完成上。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-180">If it does that, it will spend most of its time waiting for network calls to complete.</span></span> <span data-ttu-id="9f6ac-181">相反，它需要使用后端服务的异步调用，来并行处理消息批。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-181">Instead, it needs to process batches of messages in parallel, using asynchronous calls to the backend services.</span></span> <span data-ttu-id="9f6ac-182">我们知道，选择适当的检查点策略也很重要。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-182">As we'll see, choosing the right checkpointing strategy is also important.</span></span>

## <a name="workflow"></a><span data-ttu-id="9f6ac-183">工作流</span><span class="sxs-lookup"><span data-stu-id="9f6ac-183">Workflow</span></span>

<span data-ttu-id="9f6ac-184">我们探讨了用于读取和处理消息的三个选项：事件处理程序主机、服务总线队列和 IoTHub React 库。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-184">We looked at three options for reading and processing the messages: Event Processor Host, Service Bus queues, and the IoTHub React library.</span></span> <span data-ttu-id="9f6ac-185">我们选择了 IoTHub React，但要了解原因，我们最好是从事件处理程序主机着手。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-185">We chose IoTHub React, but to understand why, it helps to start with Event Processor Host.</span></span>

### <a name="event-processor-host"></a><span data-ttu-id="9f6ac-186">事件处理程序主机</span><span class="sxs-lookup"><span data-stu-id="9f6ac-186">Event Processor Host</span></span>

<span data-ttu-id="9f6ac-187">事件处理程序主机用于消息批处理。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-187">Event Processor Host is designed for message batching.</span></span> <span data-ttu-id="9f6ac-188">应用程序实现 `IEventProcessor` 接口，处理程序主机为事件中心内的每个分区创建一个事件处理程序实例。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-188">The application implements the `IEventProcessor` interface, and the Processor Host creates one event processor instance for each partition in the event hub.</span></span> <span data-ttu-id="9f6ac-189">然后，事件处理程序主机对事件消息批调用每个事件处理程序的 `ProcessEventsAsync` 方法。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-189">The Event Processor Host then calls each event processor's `ProcessEventsAsync` method with batches of event messages.</span></span> <span data-ttu-id="9f6ac-190">应用程序控制何时在 `ProcessEventsAsync` 方法内部设置检查点，事件处理程序主机将检查点写入 Azure 存储。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-190">The application controls when to checkpoint inside the `ProcessEventsAsync` method, and the Event Processor Host writes the checkpoints to Azure storage.</span></span>

<span data-ttu-id="9f6ac-191">在分区中，事件处理程序主机等待 `ProcessEventsAsync` 返回，然后对下一批再次发出调用。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-191">Within a partition, Event Processor Host waits for `ProcessEventsAsync` to return before calling again with the next batch.</span></span> <span data-ttu-id="9f6ac-192">此方法简化了编程模型，因为事件处理代码不需要可重入。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-192">This approach simplifies the programming model, because your event processing code doesn't need to be reentrant.</span></span> <span data-ttu-id="9f6ac-193">但是，它也意味着，事件处理程序每次只能处理一个批，这就限制了处理程序主机输送消息的速度。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-193">However, it also means that the event processor handles one batch at a time, and this gates the speed at which the Processor Host can pump messages.</span></span>

> [!NOTE]
> <span data-ttu-id="9f6ac-194">实际上，处理程序主机并不是像阻塞线程那样处于等待状态。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-194">The Processor Host doesn't actually *wait* in the sense of blocking a thread.</span></span> <span data-ttu-id="9f6ac-195">`ProcessEventsAsync` 方法是异步的，因此处理程序主机可以在完成该方法的过程中执行其他工作。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-195">The `ProcessEventsAsync` method is asynchronous, so the Processor Host can do other work while the method is completing.</span></span> <span data-ttu-id="9f6ac-196">但是，只有在该方法返回之后，处理程序主机才传递该分区的下一批消息。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-196">But it won't deliver another batch of messages for that partition until the method returns.</span></span>

<span data-ttu-id="9f6ac-197">在无人机应用程序中，可以并行处理一批消息。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-197">In the drone application, a batch of messages can be processed in parallel.</span></span> <span data-ttu-id="9f6ac-198">但是，等待整个批完成仍可能造成瓶颈。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-198">But waiting for the whole batch to complete can still cause a bottleneck.</span></span> <span data-ttu-id="9f6ac-199">最快的处理速度以批中最慢的消息为准。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-199">Processing can only be as fast as the slowest message within a batch.</span></span> <span data-ttu-id="9f6ac-200">响应时间出现任何差异都可能造成“长尾”，即，少数较慢的响应会拖慢整个系统。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-200">Any variation in response times can create a "long tail," where a few slow responses drag down the entire system.</span></span> <span data-ttu-id="9f6ac-201">我们的性能测试表明，使用这种方法无法实现目标吞吐量。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-201">Our performance tests showed that we did not achieve our target throughput using this approach.</span></span> <span data-ttu-id="9f6ac-202">这并不意味着我们要避免使用事件处理程序主机。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-202">This does *not* mean that you should avoid using Event Processor Host.</span></span> <span data-ttu-id="9f6ac-203">但是，为了获得较高吞吐量，应避免在 `ProcessEventsAsync` 方法中执行任何长时间运行的任务。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-203">But for high throughput, avoid doing any long-running tasks inside the `ProcessEventsAsync` method.</span></span> <span data-ttu-id="9f6ac-204">快速处理每个批。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-204">Process each batch quickly.</span></span>

### <a name="iothub-react"></a><span data-ttu-id="9f6ac-205">IotHub React</span><span class="sxs-lookup"><span data-stu-id="9f6ac-205">IotHub React</span></span>

<span data-ttu-id="9f6ac-206">[IotHub React](https://github.com/Azure/toketi-iothubreact) 是用于从事件中心读取事件的 Akka Streams 库。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-206">[IotHub React](https://github.com/Azure/toketi-iothubreact) is an Akka Streams library for reading events from Event Hub.</span></span> <span data-ttu-id="9f6ac-207">Akka Streams 是实施[反应流](https://www.reactive-streams.org/)规范的基于流的编程框架。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-207">Akka Streams is a stream-based programming framework that implements the [Reactive Streams](https://www.reactive-streams.org/) specification.</span></span> <span data-ttu-id="9f6ac-208">使用该库能够生成高效的流式处理管道，其中的所有流式处理操作以异步方式执行，并且管道能够合理处理反压。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-208">It provides a way to build efficient streaming pipelines, where all streaming operations are performed asynchronously, and the pipeline gracefully handles backpressure.</span></span> <span data-ttu-id="9f6ac-209">当事件源生成事件的速率超过下游使用者接收这些事件的速率时，就会出现反压 &mdash; 无人机交付系统遇到流量高峰时，正好也会出现这种情况。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-209">Backpressure occurs when an event source produces events at a faster rate than the downstream consumers can receive them &mdash; which is exactly the situation when the drone delivery system has a spike in traffic.</span></span> <span data-ttu-id="9f6ac-210">如果后端服务运行缓慢，IoTHub React 的速度将会下降。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-210">If backend services go slower, IoTHub React will slow down.</span></span> <span data-ttu-id="9f6ac-211">如果增加了容量，则 IoTHub React 会通过管道推送更多消息。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-211">If capacity is increased, IoTHub React will push more messages through the pipeline.</span></span>

<span data-ttu-id="9f6ac-212">Akka Streams 也是一个用于从事件中心流式处理事件的十分自然的编程模型。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-212">Akka Streams is also a very natural programming model for streaming events from Event Hubs.</span></span> <span data-ttu-id="9f6ac-213">我们无需循环访问一批事件，而可以定义一组要应用到每个事件的操作，然后让 Akka Streams 来处理流。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-213">Instead of looping through a batch of events, you define a set of operations that will be applied to each event, and let Akka Streams handle the streaming.</span></span> <span data-ttu-id="9f6ac-214">Akka Streams 在“源”、“流”和“接收器”方面定义流式处理管道。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-214">Akka Streams defines a streaming pipeline in terms of *Sources*, *Flows*, and *Sinks*.</span></span> <span data-ttu-id="9f6ac-215">源生成输出流，流处理输入流并生成输出流，接收器使用流且不生成任何输出。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-215">A source generates an output stream, a flow processes an input stream and produces an output stream, and a sink consumes a stream without producing any output.</span></span>

<span data-ttu-id="9f6ac-216">下面是计划程序服务中用于设置 Akka Streams 管道的代码：</span><span class="sxs-lookup"><span data-stu-id="9f6ac-216">Here is the code in the Scheduler service that sets up the Akka Streams pipeline:</span></span>

```java
IoTHub iotHub = new IoTHub();
Source<MessageFromDevice, NotUsed> messages = iotHub.source(options);

messages.map(msg -> DeliveryRequestEventProcessor.parseDeliveryRequest(msg))
        .filter(ad -> ad.getDelivery() != null).via(deliveryProcessor()).to(iotHub.checkpointSink())
        .run(streamMaterializer);
```

<span data-ttu-id="9f6ac-217">此代码将事件中心配置为源。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-217">This code configures Event Hubs as a source.</span></span> <span data-ttu-id="9f6ac-218">`map` 语句将每条事件消息反序列化为表示传递请求的 Java 类。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-218">The `map` statement deserializes each event message into a Java class that represents a delivery request.</span></span> <span data-ttu-id="9f6ac-219">`filter` 语句从流中删除所有 `null` 对象；这可以防止出现无法反序列化某条消息的情况。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-219">The `filter` statement removes any `null` objects from the stream; this guards against the case where a message can't be deserialized.</span></span> <span data-ttu-id="9f6ac-220">`via` 语句将源联接到用于处理每个传递请求的流。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-220">The `via` statement joins the source to a flow that processes each delivery request.</span></span> <span data-ttu-id="9f6ac-221">`to` 方法将流联接到 IoTHub React 中内置的检查点接收器。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-221">The `to` method joins the flow to the checkpoint sink, which is built into IoTHub React.</span></span>

<span data-ttu-id="9f6ac-222">IoTHub React 与事件主机处理程序使用的检查点策略不同。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-222">IoTHub React uses a different checkpointing strategy than Event Host Processor.</span></span> <span data-ttu-id="9f6ac-223">检查点由检查点接收器写入，这是管道中的终止阶段。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-223">Checkpoints are written by the checkpoint sink, which is the terminating stage in the pipeline.</span></span> <span data-ttu-id="9f6ac-224">Akka Streams 的设计允许在接收器写入检查点的同时，让管道继续流式处理数据。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-224">The design of Akka Streams allows the pipeline to continue streaming data while the sink is writing the checkpoint.</span></span> <span data-ttu-id="9f6ac-225">这意味着，上游处理阶段无需等待检查点设置发生。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-225">That means the upstream processing stages don't need to wait for checkpointing to happen.</span></span> <span data-ttu-id="9f6ac-226">我们可以配置为在发生超时之后，或者在处理选定数量的消息之后设置检查点。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-226">You can configure checkpointing to occur after a timeout or after a certain number of messages have been processed.</span></span>

<span data-ttu-id="9f6ac-227">`deliveryProcessor` 方法创建 Akka Streams 流：</span><span class="sxs-lookup"><span data-stu-id="9f6ac-227">The `deliveryProcessor` method creates the Akka Streams flow:</span></span>

```java
private static Flow<AkkaDelivery, MessageFromDevice, NotUsed> deliveryProcessor() {
    return Flow.of(AkkaDelivery.class).map(delivery -> {
        CompletableFuture<DeliverySchedule> completableSchedule = DeliveryRequestEventProcessor
                .processDeliveryRequestAsync(delivery.getDelivery(),
                        delivery.getMessageFromDevice().properties());

        completableSchedule.whenComplete((deliverySchedule,error) -> {
            if (error!=null){
                Log.info("failed delivery" + error.getStackTrace());
            }
            else{
                Log.info("Completed Delivery",deliverySchedule.toString());
            }

        });
        completableSchedule = null;
        return delivery.getMessageFromDevice();
    });
}
```

<span data-ttu-id="9f6ac-228">该流调用静态 `processDeliveryRequestAsync` 方法，以便对每条消息执行实际处理工作。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-228">The flow calls a static `processDeliveryRequestAsync` method that does the actual work of processing each message.</span></span>

### <a name="scaling-with-iothub-react"></a><span data-ttu-id="9f6ac-229">使用 IoTHub React 进行缩放</span><span class="sxs-lookup"><span data-stu-id="9f6ac-229">Scaling with IoTHub React</span></span>

<span data-ttu-id="9f6ac-230">计划程序服务在设计上可让每个容器实例从单个分区读取数据。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-230">The Scheduler service is designed so that each container instance reads from a single partition.</span></span> <span data-ttu-id="9f6ac-231">例如，如果事件中心包含 32 个分区，则在计划程序服务中部署 32 个副本。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-231">For example, if the Event Hub has 32 partitions, the Scheduler service is deployed with 32 replicas.</span></span> <span data-ttu-id="9f6ac-232">这就大大提高了横向缩放的灵活性。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-232">This allows for a lot of flexibility in terms of horizontal scaling.</span></span>

<span data-ttu-id="9f6ac-233">根据群集大小，群集中的某个节点上可能会运行多个计划程序服务 pod。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-233">Depending on the size of the cluster, a node in the cluster might have more than one Scheduler service pod running on it.</span></span> <span data-ttu-id="9f6ac-234">但是，如果计划程序服务需要更多资源，则可以横向扩展群集，以跨多个节点分配 pod。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-234">But if the Scheduler service needs more resources, the cluster can be scaled out, in order to distribute the pods across more nodes.</span></span> <span data-ttu-id="9f6ac-235">我们的性能测试表明，计划程序服务受限于内存和线程，因此，性能在很大程度上依赖于 VM 大小和每个节点的 pod 数目。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-235">Our performance tests showed that the Scheduler service is memory- and thread-bound, so performance depended greatly on the VM size and the number of pods per node.</span></span>

<span data-ttu-id="9f6ac-236">每个实例必须知道要从哪个事件中心分区读取数据。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-236">Each instance needs to know which Event Hubs partition to read from.</span></span> <span data-ttu-id="9f6ac-237">为了配置分区数目，我们利用了 Kubernetes 中的 [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) 资源类型。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-237">To configure the partition number, we took advantage of the [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) resource type in Kubernetes.</span></span> <span data-ttu-id="9f6ac-238">StatefulSet 中的 pod 有一个包含数字索引的永久标识符。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-238">Pods in a StatefulSet have a persistent identifier that includes a numeric index.</span></span> <span data-ttu-id="9f6ac-239">具体而言，pod 名称为 `<statefulset name>-<index>`，容器可以通过 Kubernetes [Downward API](https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/) 使用此值。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-239">Specifically, the pod name is `<statefulset name>-<index>`, and this value is available to the container through the Kubernetes [Downward API](https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/).</span></span> <span data-ttu-id="9f6ac-240">在运行时，计划程序服务将读取 pod 名称，并使用 pod 索引作为分区 ID。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-240">At run time, the Scheduler services reads the pod name and uses the pod index as the partition ID.</span></span>

<span data-ttu-id="9f6ac-241">如果需要进一步横向扩展计划程序服务，可为每个事件中心分区分配多个 pod，以便多个 pod 读取每个分区。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-241">If you needed to scale out the Scheduler service even further, you could assign more than one pod per event hub partition, so that multiple pods are reading each partition.</span></span> <span data-ttu-id="9f6ac-242">但是，在这种情况下，每个实例将读取分配的分区中的所有事件。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-242">However, in that case, each instance would read all of the events in the assigned partition.</span></span> <span data-ttu-id="9f6ac-243">为了避免重复处理，需要使用哈希算法，使每个实例跳过一部分消息。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-243">To avoid duplicate processing, you would need to use a hashing algorithm, so that each instance skips over a portion of the messages.</span></span> <span data-ttu-id="9f6ac-244">这样，多个读取器可以使用流，但每条消息只由一个实例处理。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-244">That way, multiple readers can consume the stream, but every message is processed by only one instance.</span></span>

![事件中心哈希图](./images/eventhub-hashing.png)

### <a name="service-bus-queues"></a><span data-ttu-id="9f6ac-246">服务总线队列</span><span class="sxs-lookup"><span data-stu-id="9f6ac-246">Service Bus queues</span></span>

<span data-ttu-id="9f6ac-247">我们考虑的第三种做法是将消息从事件中心复制到服务总线队列，然后让计划程序服务从服务总线读取消息。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-247">A third option that we considered was to copy messages from Event Hubs into a Service Bus queue, and then have the Scheduler service read the messages from Service Bus.</span></span> <span data-ttu-id="9f6ac-248">将传入的请求写入事件中心，目的只是为了将其复制服务总线，这一点看起来可能有点奇怪。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-248">It might seem strange to writing the incoming requests into Event Hubs only to copy them in Service Bus.</span></span>  <span data-ttu-id="9f6ac-249">但是，这里的思路是利用每个服务的不同优势：使用事件中心可以缓解流量高峰，而利用服务总线中的队列语义优势则可以通过使用者竞争模式处理工作负荷。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-249">However, the idea was to leverage the different strengths of each service: Use Event Hubs to absorb spikes of heavy traffic, while taking advantage of the queue semantics in Service Bus to process the workload with a competing consumers pattern.</span></span> <span data-ttu-id="9f6ac-250">请记住，我们在持续吞吐量方面的目标低于预期峰值负载，因此，处理服务总线队列的速度不需要与消息引流一样快。 </span><span class="sxs-lookup"><span data-stu-id="9f6ac-250">Remember that our target for sustained throughput is less than our expected peak load, so processing the Service Bus queue would not need to be as fast the message ingestion.</span></span>

<span data-ttu-id="9f6ac-251">利用此方法，我们的概念证明实施方案实现了每秒大约 4000 个操作。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-251">With this approach, our proof-of-concept implementation achieved about 4K operations per second.</span></span> <span data-ttu-id="9f6ac-252">这些测试使用了模拟后端服务，而这些服务并未执行任何实际工作，只是按固定的量增大了每个服务的延迟。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-252">These tests used mock backend services that did not do any real work, but simply added a fixed amount of latency per service.</span></span> <span data-ttu-id="9f6ac-253">请注意，我们的性能数字比服务总线的理论最大数字要小得多。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-253">Note that our performance numbers were much less than the theoretical maximum for Service Bus.</span></span> <span data-ttu-id="9f6ac-254">这种差异的可能原因包括：</span><span class="sxs-lookup"><span data-stu-id="9f6ac-254">Possible reasons for the discrepancy include:</span></span>

- <span data-ttu-id="9f6ac-255">没有为各种客户端参数提供最佳值，例如连接池限制、并行度、预提取计数和批大小。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-255">Not having optimal values for various client parameters, such as the connection pool limit, the degree of parallelization, the prefetch count, and the batch size.</span></span>

- <span data-ttu-id="9f6ac-256">网络 I/O 瓶颈。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-256">Network I/O bottlenecks.</span></span>

- <span data-ttu-id="9f6ac-257">使用 [PeekLock](/rest/api/servicebus/peek-lock-message-non-destructive-read) 模式，而不要使用 [ReceiveAndDelete](/rest/api/servicebus/receive-and-delete-message-destructive-read)，目的是确保至少传递消息一次。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-257">Use of [PeekLock](/rest/api/servicebus/peek-lock-message-non-destructive-read) mode rather than [ReceiveAndDelete](/rest/api/servicebus/receive-and-delete-message-destructive-read), which was needed to ensure at-least-once delivery of messages.</span></span>

<span data-ttu-id="9f6ac-258">进一步的性能测试可能发现了根本原因，并使我们能够解决这些问题。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-258">Further performance tests might have discovered the root cause and allowed us to resolve these issues.</span></span> <span data-ttu-id="9f6ac-259">但是，IotHub React满足我们的性能目标，因此我们选择了该选项。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-259">However, IotHub React met our performance target, so we chose that option.</span></span> <span data-ttu-id="9f6ac-260">也就是说，服务总线对于此方案是可行的选项。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-260">That said, Service Bus is a viable option for this scenario.</span></span>

## <a name="handling-failures"></a><span data-ttu-id="9f6ac-261">处理故障</span><span class="sxs-lookup"><span data-stu-id="9f6ac-261">Handling failures</span></span>

<span data-ttu-id="9f6ac-262">需要考虑三类常规故障。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-262">There are three general classes of failure to consider.</span></span>

1. <span data-ttu-id="9f6ac-263">下游服务可能出现非暂时性故障，即，不太可能会自行解决的故障。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-263">A downstream service may have a non-transient failure, which is any failure that's unlikely to go away by itself.</span></span> <span data-ttu-id="9f6ac-264">非暂时性故障包括普通的错误状态，例如，在方法中提供了无效的输入。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-264">Non-transient failures include normal error conditions, such as invalid input to a method.</span></span> <span data-ttu-id="9f6ac-265">此外，还包括应用程序代码中未经处理的异常或进程崩溃。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-265">They also include unhandled exceptions in application code or a process crashing.</span></span> <span data-ttu-id="9f6ac-266">如果发生此类错误，必须将整个业务事务标记为故障。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-266">If this type of error occurs, the entire business transaction must be marked as a failure.</span></span> <span data-ttu-id="9f6ac-267">可能需要撤消同一个事务中已成功完成的其他步骤。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-267">It may be necessary to undo other steps in the same transaction that already succeeded.</span></span> <span data-ttu-id="9f6ac-268">（请参阅下面的“补偿事务”。）</span><span class="sxs-lookup"><span data-stu-id="9f6ac-268">(See Compensating Transactions, below.)</span></span>

2. <span data-ttu-id="9f6ac-269">下游服务可能遇到网络超时等暂时性故障。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-269">A downstream service may experience a transient failure such as a network timeout.</span></span> <span data-ttu-id="9f6ac-270">通常，只需通过重试调用即可解决这些错误。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-270">These errors can often be resolved simply by retrying the call.</span></span> <span data-ttu-id="9f6ac-271">如果尝试特定的次数后操作仍然失败，则认为出现了非暂时性故障。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-271">If the operation still fails after a certain number of attempts, it's considered a non-transient failure.</span></span>

3. <span data-ttu-id="9f6ac-272">计划程序服务本身可能发生故障（例如，由于节点崩溃）。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-272">The Scheduler service itself might fault (for example, because a node crashes).</span></span> <span data-ttu-id="9f6ac-273">在这种情况下，Kubernetes 会启动服务的新实例。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-273">In that case, Kubernetes will bring up a new instance of the service.</span></span> <span data-ttu-id="9f6ac-274">但是，必须恢复已在处理的任何事务。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-274">However, any transactions that were already in progress must be resumed.</span></span>

## <a name="compensating-transactions"></a><span data-ttu-id="9f6ac-275">补偿事务</span><span class="sxs-lookup"><span data-stu-id="9f6ac-275">Compensating transactions</span></span>

<span data-ttu-id="9f6ac-276">如果发生了非暂时性故障，当前事务可能进入“部分失败”状态，此时，一个或多个步骤已成功完成。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-276">If a non-transient failure happens, the current transaction might be in a *partially failed* state, where one or more steps already completed successfully.</span></span> <span data-ttu-id="9f6ac-277">例如，如果无人机服务已安排无人机，则必须取消该无人机。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-277">For example, if the Drone service already scheduled a drone, the drone must be canceled.</span></span> <span data-ttu-id="9f6ac-278">在这种情况下，应用程序需要使用[补偿事务](../patterns/compensating-transaction.md)撤消已成功的步骤。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-278">In that case, the application needs to undo the steps that succeeded, by using a [Compensating Transaction](../patterns/compensating-transaction.md).</span></span> <span data-ttu-id="9f6ac-279">在某些情况下，必须通过外部系统甚至手动过程来执行此操作。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-279">In some cases, this must be done by an external system or even by a manual process.</span></span>

<span data-ttu-id="9f6ac-280">如果补偿事务的逻辑比较复杂，请考虑创建一个单独的服务来负责处理此过程。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-280">If the logic for compensating transactions is complex, consider creating a separate service that is responsible for this process.</span></span> <span data-ttu-id="9f6ac-281">在无人机交付应用程序中，计划程序服务会将失败的操作放入专用队列。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-281">In the Drone Delivery application, the Scheduler service puts failed operations onto a dedicated queue.</span></span> <span data-ttu-id="9f6ac-282">一个单独的微服务（称作“监督程序”）会从此队列读取数据，并针对需要补偿的服务调用取消 API。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-282">A separate microservice, called the Supervisor, reads from this queue and calls a cancellation API on the services that need to compensate.</span></span> <span data-ttu-id="9f6ac-283">这是[计划程序代理监督程序模式][scheduler-agent-supervisor]的一个变体。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-283">This is a variation of the [Scheduler Agent Supervisor pattern][scheduler-agent-supervisor].</span></span> <span data-ttu-id="9f6ac-284">监督程序服务可能还会执行其他操作，例如，通过文本或电子邮件通知用户，或将警报发送到操作仪表板。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-284">The Supervisor service might take other actions as well, such as notify the user by text or email, or send an alert to an operations dashboard.</span></span>

![显示监督程序微服务的示意图](./images/supervisor.png)

## <a name="idempotent-versus-non-idempotent-operations"></a><span data-ttu-id="9f6ac-286">幂等与非幂等操作</span><span class="sxs-lookup"><span data-stu-id="9f6ac-286">Idempotent versus non-idempotent operations</span></span>

<span data-ttu-id="9f6ac-287">为了避免丢失任何请求，计划程序服务必须保证至少处理所有消息一次。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-287">To avoid losing any requests, the Scheduler service must guarantee that all messages are processed at least once.</span></span> <span data-ttu-id="9f6ac-288">如果客户端正确设置了检查点，事件中心可以保证至少传递一次。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-288">Event Hubs can guarantee at-least-once delivery if the client checkpoints correctly.</span></span>

<span data-ttu-id="9f6ac-289">计划程序服务可能在处理一个或多个客户端请求的过程中崩溃。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-289">If the Scheduler service crashes, it may be in the middle of processing one or more client requests.</span></span> <span data-ttu-id="9f6ac-290">这些消息将由计划程序的另一个实例拾取并重新处理。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-290">Those messages will be picked up by another instance of the Scheduler and reprocessed.</span></span> <span data-ttu-id="9f6ac-291">如果处理某个请求两次，会发生什么情况？</span><span class="sxs-lookup"><span data-stu-id="9f6ac-291">What happens if a request is processed twice?</span></span> <span data-ttu-id="9f6ac-292">必须避免重复任何工作。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-292">It's important to avoid duplicating any work.</span></span> <span data-ttu-id="9f6ac-293">毕竟，我们不希望系统派遣两架无人机来投递同一个包裹。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-293">After all, we don't want the system to send two drones for the same package.</span></span>

<span data-ttu-id="9f6ac-294">一种做法是将所有操作设计为幂等。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-294">One approach is to design all operations to be idempotent.</span></span> <span data-ttu-id="9f6ac-295">如果某个操作可以调用多次，且在首次调用后不会产生其他副作用，则该操作是幂等的。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-295">An operation is idempotent if it can be called multiple times without producing additional side-effects after the first call.</span></span> <span data-ttu-id="9f6ac-296">换而言之，客户端可以调用该操作一次、两次或许多次，而结果是相同的。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-296">In other words, a client can invoke the operation once, twice, or many times, and the result will be the same.</span></span> <span data-ttu-id="9f6ac-297">从根本上讲，服务应忽略重复调用。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-297">Essentially, the service should ignore duplicate calls.</span></span> <span data-ttu-id="9f6ac-298">要使产生副作用的操作成为幂等操作，服务必须能够检测重复调用。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-298">For a method with side effects to be idempotent, the service must be able to detect duplicate calls.</span></span> <span data-ttu-id="9f6ac-299">例如，你可以让调用方分配 ID，而不要让服务生成新 ID。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-299">For example, you can have the caller assign the ID, rather than having the service generate a new ID.</span></span> <span data-ttu-id="9f6ac-300">然后，服务可以检查重复 ID。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-300">The service can then check for duplicate IDs.</span></span>

> [!NOTE]
> <span data-ttu-id="9f6ac-301">HTTP 规范中规定，GET、PUT 和 DELETE 方法必须是幂等的。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-301">The HTTP specification states that GET, PUT, and DELETE methods must be idempotent.</span></span> <span data-ttu-id="9f6ac-302">无法保证 POST 方法是幂等的。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-302">POST methods are not guaranteed to be idempotent.</span></span> <span data-ttu-id="9f6ac-303">如果 POST 方法创建新资源，则通常无法保证此操作是幂等的。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-303">If a POST method creates a new resource, there is generally no guarantee that this operation is idempotent.</span></span>

<span data-ttu-id="9f6ac-304">编写幂等方法并不总是那么直截了当。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-304">It's not always straightforward to write idempotent method.</span></span> <span data-ttu-id="9f6ac-305">另一种做法是让计划程序跟踪持久性存储中每个事务的进度。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-305">Another option is for the Scheduler to track the progress of every transaction in a durable store.</span></span> <span data-ttu-id="9f6ac-306">每当计划程序处理一条消息，它都会在持久性存储中查找状态。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-306">Whenever it processes a message, it would look up the state in the durable store.</span></span> <span data-ttu-id="9f6ac-307">完成每个步骤后，它会将结果写入存储。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-307">After each step, it would write the result to the store.</span></span> <span data-ttu-id="9f6ac-308">此方法可能对性能造成影响。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-308">There may be performance implications to this approach.</span></span>

## <a name="example-idempotent-operations"></a><span data-ttu-id="9f6ac-309">示例：幂等操作</span><span class="sxs-lookup"><span data-stu-id="9f6ac-309">Example: Idempotent operations</span></span>

<span data-ttu-id="9f6ac-310">HTTP 规范中规定，PUT 方法必须是幂等的。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-310">The HTTP specification states that PUT methods must be idempotent.</span></span> <span data-ttu-id="9f6ac-311">该规范对幂等的定义如下：</span><span class="sxs-lookup"><span data-stu-id="9f6ac-311">The specification defines idempotent this way:</span></span>

> <span data-ttu-id="9f6ac-312">如果使用某个请求方法的多个相同请求对服务器造成的预期影响与单个此类请求所造成的影响相同，则认为该方法是“幂等的”。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-312">A request method is considered "idempotent" if the intended effect on the server of multiple identical requests with that method is the same as the effect for a single such request.</span></span> <span data-ttu-id="9f6ac-313">([RFC 7231](https://tools.ietf.org/html/rfc7231#section-4))</span><span class="sxs-lookup"><span data-stu-id="9f6ac-313">([RFC 7231](https://tools.ietf.org/html/rfc7231#section-4))</span></span>

<span data-ttu-id="9f6ac-314">创建新实体时，必须了解 PUT 与 POST 语义之间的区别。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-314">It's important to understand the difference between PUT and POST semantics when creating a new entity.</span></span> <span data-ttu-id="9f6ac-315">在这两种情况下，客户端都会在请求正文中发送实体的表示形式。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-315">In both cases, the client sends a representation of an entity in the request body.</span></span> <span data-ttu-id="9f6ac-316">但 URI 的含义有所不同。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-316">But the meaning of the URI is different.</span></span>

- <span data-ttu-id="9f6ac-317">对于 POST 方法，URI 表示新实体的父资源，例如集合。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-317">For a POST method, the URI represents a parent resource of the new entity, such as a collection.</span></span> <span data-ttu-id="9f6ac-318">例如，若要创建新的交付项，URI 可能是 `/api/deliveries`。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-318">For example, to create a new delivery, the URI might be `/api/deliveries`.</span></span> <span data-ttu-id="9f6ac-319">服务器将创建实体并为其分配新 URI，例如 `/api/deliveries/39660`。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-319">The server creates the entity and assigns it a new URI, such as `/api/deliveries/39660`.</span></span> <span data-ttu-id="9f6ac-320">此 URI 将在响应的 Location 标头中返回。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-320">This URI is returned in the Location header of the response.</span></span> <span data-ttu-id="9f6ac-321">每当客户端发送请求时，服务器都会创建一个具有新 URI 的新实体。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-321">Each time the client sends a request, the server will create a new entity with a new URI.</span></span>

- <span data-ttu-id="9f6ac-322">对于 PUT 方法，URI 标识实体。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-322">For a PUT method, the URI identifies the entity.</span></span> <span data-ttu-id="9f6ac-323">如果已存在具有该 URI 的实体，则服务器会将现有实体替换为请求中的版本。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-323">If there already exists an entity with that URI, the server replaces the existing entity with the version in the request.</span></span> <span data-ttu-id="9f6ac-324">如果不存在具有该 URI 的实体，则服务器会创建一个实体。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-324">If no entity exists with that URI, the server creates one.</span></span> <span data-ttu-id="9f6ac-325">例如，假设客户端向 `api/deliveries/39660` 发送了 PUT 请求。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-325">For example, suppose the client sends a PUT request to `api/deliveries/39660`.</span></span> <span data-ttu-id="9f6ac-326">此外，假设不存在具有该 URI 的交付项，则服务器会创建一个新的交付项。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-326">Assuming there is no delivery with that URI, the server creates a new one.</span></span> <span data-ttu-id="9f6ac-327">现在，如果客户端再次发送相同的请求，则服务器会替换现有实体。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-327">Now if the client sends the same request again, the server will replace the existing entity.</span></span>

<span data-ttu-id="9f6ac-328">下面交付服务的 PUT 方法实现。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-328">Here is the Delivery service's implementation of the PUT method.</span></span>

```csharp
[HttpPut("{id}")]
[ProducesResponseType(typeof(Delivery), 201)]
[ProducesResponseType(typeof(void), 204)]
public async Task<IActionResult> Put([FromBody]Delivery delivery, string id)
{
    logger.LogInformation("In Put action with delivery {Id}: {@DeliveryInfo}", id, delivery.ToLogInfo());
    try
    {
        var internalDelivery = delivery.ToInternal();

        // Create the new delivery entity.
        await deliveryRepository.CreateAsync(internalDelivery);

        // Create a delivery status event.
        var deliveryStatusEvent = new DeliveryStatusEvent { DeliveryId = delivery.Id, Stage = DeliveryEventType.Created };
        await deliveryStatusEventRepository.AddAsync(deliveryStatusEvent);

        // Return HTTP 201 (Created)
        return CreatedAtRoute("GetDelivery", new { id= delivery.Id }, delivery);
    }
    catch (DuplicateResourceException)
    {
        // This method is mainly used to create deliveries. If the delivery already exists then update it.
        logger.LogInformation("Updating resource with delivery id: {DeliveryId}", id);

        var internalDelivery = delivery.ToInternal();
        await deliveryRepository.UpdateAsync(id, internalDelivery);

        // Return HTTP 204 (No Content)
        return NoContent();
    }
}
```

<span data-ttu-id="9f6ac-329">大多数请求预期会创建新实体，因此，该方法将对存储库对象乐观调用 `CreateAsync`，然后通过更新资源来处理任何重复资源异常。</span><span class="sxs-lookup"><span data-stu-id="9f6ac-329">It's expected that most requests will create a new entity, so the method optimistically calls `CreateAsync` on the repository object, and then handles any duplicate-resource exceptions by updating the resource instead.</span></span>

<!-- links -->

[scheduler-agent-supervisor]: ../patterns/scheduler-agent-supervisor.md